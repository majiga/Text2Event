{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification\n",
    "Source:\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data set\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 NEWS CATEGORIES:\n",
      "\n",
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n",
      "\n",
      "TRAINING DATA :\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LABELS:  [7]\n"
     ]
    }
   ],
   "source": [
    "# Check the labels (categories) and some data files in the TRAINING data\n",
    "print('20 NEWS CATEGORIES:\\n', *twenty_train.target_names, sep='\\n') #prints all the categories\n",
    "print('\\nTRAINING DATA :\\n', *twenty_train.data[:1], sep='\\n')\n",
    "print('LABELS: ', twenty_train.target[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATA:\n",
      "\n",
      "From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n",
      "LABELS:  [7]\n"
     ]
    }
   ],
   "source": [
    "# Check the labels (categories) and some data files in the TEST data\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "print('TEST DATA:\\n', *twenty_test.data[:1], sep='\\n')\n",
    "print('LABELS: ', twenty_test.target[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML algorithms\n",
    "Source: https://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet-2.png\n",
    "\n",
    "<img src=\"https://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet-2.png\" width=1200 height=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features\n",
    "- CountVectorizer: Text files are actually series of words (ordered). In order to run machine learning algorithms we need to convert the text files into numerical feature vectors. We will be using bag of words model for our example. We segment each text file into words (splitting by space), and count the number of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).\n",
    "\n",
    "- TfidfTransformer: Counting the number of words in each document will give more weightage to longer documents than shorter documents. To avoid this issue, we can use TF (Term Frequencies) i.e. #count(word) / #Total words, in each document. Moreover, to reduce the weightage of more common words like (the, is, an, etc.) in all document, TF-IDF i.e Term Frequency times Inverse Document Frequency is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The simplest text classifier is Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline to do count vectorisation, TF-IDF transformation, and Naive Bayes classification.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), # learns the vocabulary dictionary & returns a Document-Term matrix. [n_samples, n_features]\n",
    "                     ('tfidf', TfidfTransformer()), # Term Frequency - Inverse Document Frequency\n",
    "                     ('clf', MultinomialNB())]) # Naive Bayes classifier\n",
    "\n",
    "# train the NB classifier on the training data\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measurements on the test data with Accuracy, Precision, Recall, and F1 metrics\n",
    "\n",
    "- Accuracy is a ratio of correctly predicted observation to the total observations.\n",
    "- Precision defines how precise/accurate your model is out of those predicted positive, how many of them are actual positive.\n",
    "- Recall calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). \n",
    "- F1 Score is a better measure to balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Accuracy score:  0.7738980350504514\n",
      "NB Precision score:  0.8218781741893993\n",
      "NB Recall score:  0.7738980350504514\n",
      "NB F1 score:  0.7684457156894653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score ,f1_score, classification_report\n",
    "\n",
    "print('NB Accuracy score: ', accuracy_score(twenty_test.target, predicted)) # same as np.mean(predicted == twenty_test.target)\n",
    "print('NB Precision score: ', precision_score(twenty_test.target, predicted, average='weighted'))\n",
    "print('NB Recall score: ', recall_score(twenty_test.target, predicted, average='weighted'))\n",
    "print('NB F1 score: ', f1_score(twenty_test.target, predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 score:  0.8179850964920279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=42)), ])\n",
    "\n",
    "text_clf_svm.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
    "\n",
    "#accuracy_svm = np.mean(predicted_svm == twenty_test.target)\n",
    "#print('SVM accuracy = ', accuracy_svm)\n",
    "print('SVM F1 score: ', f1_score(twenty_test.target, predicted_svm, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-nearest neighbors algorithm (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier F1 score:  0.6597157454309466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_kneighbors = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier()),\n",
    "                     ])\n",
    "text_kneighbors.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_kneigbor = text_kneighbors.predict(twenty_test.data)\n",
    "\n",
    "print('KNeighborsClassifier F1 score: ', f1_score(twenty_test.target, predicted_kneigbor, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desicion tree F1 score:  0.5607868118351039\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "text_tree = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', tree.DecisionTreeClassifier()),\n",
    "                     ])\n",
    "text_tree.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_tree = text_tree.predict(twenty_test.data)\n",
    "\n",
    "print('Desicion tree F1 score: ', f1_score(twenty_test.target, predicted_tree, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random forests is an ensemble learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majiga/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier F1 score:  0.7518869831313583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_rfc = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "                     ])\n",
    "text_rfc.fit(twenty_train.data, twenty_train.target)\n",
    "predicted_rfc = text_rfc.predict(twenty_test.data)\n",
    "\n",
    "# print(classification_report(twenty_test.target, predicted_rfc))\n",
    "print('RandomForestClassifier F1 score: ', f1_score(twenty_test.target, predicted_rfc, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.65      0.69       319\n",
      "          1       0.55      0.70      0.62       389\n",
      "          2       0.65      0.75      0.70       394\n",
      "          3       0.64      0.63      0.63       392\n",
      "          4       0.70      0.76      0.73       385\n",
      "          5       0.77      0.69      0.72       395\n",
      "          6       0.74      0.92      0.82       390\n",
      "          7       0.78      0.78      0.78       396\n",
      "          8       0.90      0.90      0.90       398\n",
      "          9       0.80      0.90      0.85       397\n",
      "         10       0.90      0.94      0.92       399\n",
      "         11       0.89      0.91      0.90       396\n",
      "         12       0.73      0.52      0.61       393\n",
      "         13       0.84      0.65      0.74       396\n",
      "         14       0.82      0.87      0.84       394\n",
      "         15       0.69      0.91      0.79       398\n",
      "         16       0.66      0.85      0.74       364\n",
      "         17       0.95      0.80      0.87       376\n",
      "         18       0.81      0.46      0.59       310\n",
      "         19       0.81      0.31      0.45       251\n",
      "\n",
      "avg / total       0.77      0.76      0.75      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you want the performance by categories, classification_report can be used\n",
    "print(classification_report(twenty_test.target, predicted_rfc)) # RandomForestClassifier performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
